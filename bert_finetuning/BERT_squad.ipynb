{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_squad",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAUq7duyG5J0"
      },
      "source": [
        "# Stage 1: Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XYlThAmGZg1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ab8307f-c3fd-4da5-e4a3-21bdf0a1d95d"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install tf-models-official\n",
        "#!pip install tf-models-nightly # better to install the version in development\n",
        "!pip install tf-nightly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 14.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Collecting tf-models-official\n",
            "  Downloading tf_models_official-2.8.0-py2.py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 14.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (5.4.8)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.4.1)\n",
            "Collecting tensorflow-text~=2.8.0\n",
            "  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 74.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.29.27)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.15.0)\n",
            "Collecting tf-slim>=1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 71.8 MB/s \n",
            "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.19.5)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.5.0)\n",
            "Collecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 62.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.12.10)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.1.96)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.0.1)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.1-py2.py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 78.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (2.0.4)\n",
            "Collecting tensorflow~=2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 497.5 MB 25 kB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (7.1.2)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.7 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.1.3)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.5.12)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.12.0)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.26.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.35.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.17.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (1.54.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.8)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (5.0.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (2021.10.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.24.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.0.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.0.4)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (0.24.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.13.3)\n",
            "Collecting numpy>=1.15.4\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (0.4.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 69.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (3.10.0.2)\n",
            "Collecting keras<2.9,>=2.8.0rc0\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 58.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.1.0)\n",
            "Collecting tensorboard<2.9,>=2.8\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.43.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (13.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-models-official) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.8.0->tf-models-official) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (0.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (3.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (3.2.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (1.3.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official) (2019.12.20)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official) (0.8.9)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (5.4.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.16.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (21.4.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.3.4)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (1.6.0)\n",
            "Building wheels for collected packages: py-cpuinfo, seqeval\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=022bbfd244fc979f2986bdee0fbeafbef1592ac579893a023cff53ddbb6c18a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=f2750664975b6a8a38c10930c72b624bf99a30d40aa576a9c3c7f8beae8e5968\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built py-cpuinfo seqeval\n",
            "Installing collected packages: numpy, tf-estimator-nightly, tensorboard, keras, tensorflow, portalocker, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sacrebleu, pyyaml, py-cpuinfo, opencv-python-headless, tf-models-official\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed colorama-0.4.4 keras-2.8.0 numpy-1.21.5 opencv-python-headless-4.5.5.62 portalocker-2.3.2 py-cpuinfo-8.0.0 pyyaml-5.4.1 sacrebleu-2.0.0 seqeval-1.2.2 tensorboard-2.8.0 tensorflow-2.8.0 tensorflow-addons-0.15.0 tensorflow-model-optimization-0.7.1 tensorflow-text-2.8.1 tf-estimator-nightly-2.8.0.dev2021122109 tf-models-official-2.8.0 tf-slim-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-nightly\n",
            "  Downloading tf_nightly-2.9.0.dev20220212-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (497.7 MB)\n",
            "\u001b[K     |███████████████████████▌        | 365.7 MB 39.1 MB/s eta 0:00:04"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDdYqvxPHnI8"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFS9XSASHvQi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "76439cc2-526a-47c8-dac2-0b9e941d67ba"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.8.0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv38eJzSH00S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40265aea-154c-411e-f23c-fa0157e4cb6c"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "from official.nlp.bert.tokenization import FullTokenizer\n",
        "from official.nlp.bert.input_pipeline import create_squad_dataset\n",
        "from official.nlp.data.squad_lib import generate_tf_record_from_json_file\n",
        "\n",
        "from official.nlp import optimization\n",
        "\n",
        "from official.nlp.data.squad_lib import read_squad_examples\n",
        "from official.nlp.data.squad_lib import FeatureWriter\n",
        "from official.nlp.data.squad_lib import convert_examples_to_features\n",
        "from official.nlp.data.squad_lib import write_predictions"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjzrCZJMJN1N"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import collections\n",
        "import os\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSGw1I_zHAb5"
      },
      "source": [
        "# Stage 2: Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfdTEReCKFky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79878ba4-8b42-4a60-d943-937f2642b3f0"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_PcOeemKcYq"
      },
      "source": [
        "input_meta_data = generate_tf_record_from_json_file(\n",
        "    \"/content/drive/MyDrive/Datasets/Squad_1.0/train-v1.1.json\",\n",
        "    \"/content/drive/MyDrive/Datasets/Squad_1.0/vocab.txt\",\n",
        "    \"/content/drive/MyDrive/Datasets/Squad_1.0/train-v1.1.tf_record\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(depth=3)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Datasets/Squad_1.0/train-v1.1.json\") as f:\n",
        "  data = json.load(f)['data']\n",
        "  num_topics = len(data)\n",
        "  print(f\"#Topics: {num_topics}\")\n",
        "\n",
        "  total_training_ex = 0\n",
        "  print_ex = True\n",
        "\n",
        "  for topic_idx in range(num_topics):\n",
        "    topic = data[topic_idx]\n",
        "    title = topic['title']\n",
        "    paras = topic['paragraphs']\n",
        "    num_paras = len(paras)\n",
        "    \n",
        "    if print_ex:\n",
        "      print(f\"[{topic_idx}] Topic Title : {title}\")\n",
        "      print(f\"#Paragraphs : {num_paras}\")\n",
        "    \n",
        "    for para_idx in range(num_paras):\n",
        "      para = paras[para_idx]\n",
        "      qas = para['qas']\n",
        "      num_ques = len(qas)\n",
        "\n",
        "      if print_ex:\n",
        "        print(f\"[{para_idx}] Context : {para['context']}\")\n",
        "\n",
        "      for ques_idx in range(len(qas)):\n",
        "        ques = qas[ques_idx]['question']\n",
        "        ans = qas[ques_idx]['answers']\n",
        "\n",
        "        total_training_ex += len(ans)\n",
        "\n",
        "        if print_ex:\n",
        "          print(f\"Q{ques_idx + 1} : {ques}\")\n",
        "          print(f\"A{ques_idx + 1} : {ans}\")\n",
        "\n",
        "        #pp.pprint(qas)\n",
        "      print_ex = False\n",
        "    \n",
        "  \n",
        "  print(f\"No of training examples : {total_training_ex}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57l8EyEpYCak",
        "outputId": "821af83b-e085-4bf8-d65e-4cf161c0806b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Topics: 442\n",
            "[0] Topic Title : University_of_Notre_Dame\n",
            "#Paragraphs : 55\n",
            "[0] Context : Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "Q1 : To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "A1 : [{'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}]\n",
            "Q2 : What is in front of the Notre Dame Main Building?\n",
            "A2 : [{'answer_start': 188, 'text': 'a copper statue of Christ'}]\n",
            "Q3 : The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
            "A3 : [{'answer_start': 279, 'text': 'the Main Building'}]\n",
            "Q4 : What is the Grotto at Notre Dame?\n",
            "A4 : [{'answer_start': 381, 'text': 'a Marian place of prayer and reflection'}]\n",
            "Q5 : What sits on top of the Main Building at Notre Dame?\n",
            "A5 : [{'answer_start': 92, 'text': 'a golden statue of the Virgin Mary'}]\n",
            "No of training examples : 87599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XQGdnANLziM"
      },
      "source": [
        "with tf.io.gfile.GFile(\"/content/drive/MyDrive/Datasets/Squad_1.0/train_meta_data\", \"w\") as writer:\n",
        "    writer.write(json.dumps(input_meta_data, indent=4) + \"\\n\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.io.gfile.GFile(\"/content/drive/MyDrive/Datasets/Squad_1.0/train_meta_data\", \"r\") as r:\n",
        "    input_meta_data = json.load(r)"
      ],
      "metadata": {
        "id": "3TwYuVvXiKAc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(input_meta_data, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noKX57OiA_KL",
        "outputId": "451c900c-f846-4ab1-dfed-48afb3eca74d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"task_type\": \"bert_squad\",\n",
            "    \"train_data_size\": 88641,\n",
            "    \"max_seq_length\": 384,\n",
            "    \"max_query_length\": 64,\n",
            "    \"doc_stride\": 128,\n",
            "    \"version_2_with_negative\": false\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295nG2zQMYSU"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "train_dataset = create_squad_dataset(\n",
        "    \"/content/drive/MyDrive/Datasets/Squad_1.0/train-v1.1.tf_record\",\n",
        "    input_meta_data['max_seq_length'], # 384\n",
        "    BATCH_SIZE,\n",
        "    is_training=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4939RJqHRUs"
      },
      "source": [
        "# Stage 3: Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcQcFi8kOc6K"
      },
      "source": [
        "## Squad layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjmLeQjiaOo5"
      },
      "source": [
        "class BertSquadLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(BertSquadLayer, self).__init__()\n",
        "    self.final_dense = tf.keras.layers.Dense(\n",
        "        units=2,\n",
        "        kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    logits = self.final_dense(inputs) # (batch_size, seq_len, 2)\n",
        "\n",
        "    logits = tf.transpose(logits, [2, 0, 1]) # (2, batch_size, seq_len)\n",
        "    unstacked_logits = tf.unstack(logits, axis=0) # [(batch_size, seq_len), (batch_size, seq_len)] \n",
        "    return unstacked_logits[0], unstacked_logits[1]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQbQFKjUOeyf"
      },
      "source": [
        "## Whole model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgkIFb1GMT81"
      },
      "source": [
        "class BERTSquad(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 name=\"bert_squad\"):\n",
        "        super(BERTSquad, self).__init__(name=name)\n",
        "        \n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=True)\n",
        "        \n",
        "        self.squad_layer = BertSquadLayer()\n",
        "    \n",
        "    def apply_bert(self, inputs):\n",
        "#        _ , sequence_output = self.bert_layer([inputs[\"input_ids\"],\n",
        "#                                               inputs[\"input_mask\"],\n",
        "#                                               inputs[\"segment_ids\"]])\n",
        "        \n",
        "        # New names for the 3 different elements of the inputs, since an update\n",
        "        # in tf_models_officials. Doesn't change anything for any other BERT\n",
        "        # usage.\n",
        "        _ , sequence_output = self.bert_layer([inputs[\"input_word_ids\"],\n",
        "                                               inputs[\"input_mask\"],\n",
        "                                               inputs[\"input_type_ids\"]])\n",
        "        return sequence_output\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_output = self.apply_bert(inputs)\n",
        "\n",
        "        start_logits, end_logits = self.squad_layer(seq_output)\n",
        "        \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBmSU2RnHV_a"
      },
      "source": [
        "# Stage 4: Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnA3WHwRIHAZ"
      },
      "source": [
        "## Creating the AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEUxomxENRoJ"
      },
      "source": [
        "TRAIN_DATA_SIZE = 88641\n",
        "NB_BATCHES_TRAIN = 2000\n",
        "BATCH_SIZE = 4\n",
        "NB_EPOCHS = 3\n",
        "INIT_LR = 5e-5\n",
        "WARMUP_STEPS = int(NB_BATCHES_TRAIN * 0.1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pg6EKe2daFy"
      },
      "source": [
        "train_dataset_light = train_dataset.take(NB_BATCHES_TRAIN)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHd5MzTdNIZq"
      },
      "source": [
        "bert_squad = BERTSquad()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0cvDjBm_KXT"
      },
      "source": [
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=INIT_LR,\n",
        "    num_train_steps=NB_BATCHES_TRAIN,\n",
        "    num_warmup_steps=WARMUP_STEPS)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6kG-HpzVK7v"
      },
      "source": [
        "def squad_loss_fn(labels, model_outputs):\n",
        "    start_positions = labels['start_positions']\n",
        "    end_positions = labels['end_positions']\n",
        "    start_logits, end_logits = model_outputs\n",
        "\n",
        "    start_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        start_positions, start_logits, from_logits=True)\n",
        "    end_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        end_positions, end_logits, from_logits=True)\n",
        "    \n",
        "    total_loss = (tf.reduce_mean(start_loss) + tf.reduce_mean(end_loss)) / 2\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN_C_WA5R_Cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada0ec88-7eb0-41e8-e413-8b1fca62bf3b"
      },
      "source": [
        "next(iter(train_dataset_light))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'input_mask': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_type_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_word_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[ 101, 2105, 2054, ...,    0,    0,    0],\n",
              "         [ 101, 2029, 2533, ...,    0,    0,    0],\n",
              "         [ 101, 2129, 2116, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 7017, ...,    0,    0,    0]], dtype=int32)>},\n",
              " {'end_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([202, 172,  32, 191], dtype=int32)>,\n",
              "  'start_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([201, 167,  29, 188], dtype=int32)>})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-iE2QFC_KRI"
      },
      "source": [
        "bert_squad.compile(optimizer,\n",
        "                   squad_loss_fn)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjp-_4OyTbuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59f0b231-9c78-442c-cbbc-bf92a29778a3"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(bert_squad=bert_squad)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest checkpoint restored!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDgEq09xIOOl"
      },
      "source": [
        "## Custom training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "q9dhBd0jipwu",
        "outputId": "282ad1a6-ce37-4635-9515-7f947f4d82e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Feb 12 20:06:04 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    27W /  70W |   2342MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ywelW3uaSbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9fcb3b-31c9-48be-e4f5-10fc77b14329"
      },
      "source": [
        "for epoch in range(NB_EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    \n",
        "    for (batch, (inputs, targets)) in enumerate(train_dataset_light):\n",
        "        with tf.GradientTape() as tape:\n",
        "            model_outputs = bert_squad(inputs)\n",
        "            loss = squad_loss_fn(targets, model_outputs)\n",
        "        \n",
        "        gradients = tape.gradient(loss, bert_squad.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, bert_squad.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result()))\n",
        "        \n",
        "        if batch % 500 == 0:\n",
        "            ckpt_save_path = ckpt_manager.save()\n",
        "            print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                                ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 5.9036\n",
            "Saving checkpoint for epoch 1 at /content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/ckpt-2\n",
            "Epoch 1 Batch 50 Loss 5.7067\n",
            "Epoch 1 Batch 100 Loss 5.1587\n",
            "Epoch 1 Batch 150 Loss 4.4845\n",
            "Epoch 1 Batch 200 Loss 4.0156\n",
            "Epoch 1 Batch 250 Loss 3.6229\n",
            "Epoch 1 Batch 300 Loss 3.3475\n",
            "Epoch 1 Batch 350 Loss 3.1951\n",
            "Epoch 1 Batch 400 Loss 3.0206\n",
            "Epoch 1 Batch 450 Loss 2.8601\n",
            "Epoch 1 Batch 500 Loss 2.7166\n",
            "Saving checkpoint for epoch 1 at /content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/ckpt-3\n",
            "Epoch 1 Batch 550 Loss 2.5959\n",
            "Epoch 1 Batch 600 Loss 2.5263\n",
            "Epoch 1 Batch 650 Loss 2.4543\n",
            "Epoch 1 Batch 700 Loss 2.3852\n",
            "Epoch 1 Batch 750 Loss 2.3227\n",
            "Epoch 1 Batch 800 Loss 2.2554\n",
            "Epoch 1 Batch 850 Loss 2.2245\n",
            "Epoch 1 Batch 900 Loss 2.1821\n",
            "Epoch 1 Batch 950 Loss 2.1351\n",
            "Epoch 1 Batch 1000 Loss 2.0768\n",
            "Saving checkpoint for epoch 1 at /content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/ckpt-4\n",
            "Epoch 1 Batch 1050 Loss 2.0312\n",
            "Epoch 1 Batch 1100 Loss 1.9758\n",
            "Epoch 1 Batch 1150 Loss 1.9271\n",
            "Epoch 1 Batch 1200 Loss 1.8939\n",
            "Epoch 1 Batch 1250 Loss 1.8754\n",
            "Epoch 1 Batch 1300 Loss 1.8628\n",
            "Epoch 1 Batch 1350 Loss 1.8544\n",
            "Epoch 1 Batch 1400 Loss 1.8351\n",
            "Epoch 1 Batch 1450 Loss 1.8101\n",
            "Epoch 1 Batch 1500 Loss 1.7844\n",
            "Saving checkpoint for epoch 1 at /content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/ckpt-5\n",
            "Epoch 1 Batch 1550 Loss 1.7656\n",
            "Epoch 1 Batch 1600 Loss 1.7481\n",
            "Epoch 1 Batch 1650 Loss 1.7430\n",
            "Epoch 1 Batch 1700 Loss 1.7349\n",
            "Epoch 1 Batch 1750 Loss 1.7222\n",
            "Epoch 1 Batch 1800 Loss 1.7114\n",
            "Epoch 1 Batch 1850 Loss 1.6855\n",
            "Epoch 1 Batch 1900 Loss 1.6613\n",
            "Epoch 1 Batch 1950 Loss 1.6461\n",
            "Time taken for 1 epoch: 908.564980506897 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 0.4728\n",
            "Saving checkpoint for epoch 2 at /content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/ckpt-6\n",
            "Epoch 2 Batch 50 Loss 1.0073\n",
            "Epoch 2 Batch 100 Loss 1.1076\n",
            "Epoch 2 Batch 150 Loss 1.0341\n",
            "Epoch 2 Batch 200 Loss 1.0720\n",
            "Epoch 2 Batch 250 Loss 0.9958\n",
            "Epoch 2 Batch 300 Loss 0.9547\n",
            "Epoch 2 Batch 350 Loss 0.9495\n",
            "Epoch 2 Batch 400 Loss 0.9143\n",
            "Epoch 2 Batch 450 Loss 0.8620\n",
            "Epoch 2 Batch 500 Loss 0.8370\n",
            "Saving checkpoint for epoch 2 at /content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/ckpt-7\n",
            "Epoch 2 Batch 550 Loss 0.8140\n",
            "Epoch 2 Batch 600 Loss 0.7926\n",
            "Epoch 2 Batch 650 Loss 0.7838\n",
            "Epoch 2 Batch 700 Loss 0.7606\n",
            "Epoch 2 Batch 750 Loss 0.7339\n",
            "Epoch 2 Batch 800 Loss 0.7221\n",
            "Epoch 2 Batch 850 Loss 0.7075\n",
            "Epoch 2 Batch 900 Loss 0.6954\n",
            "Epoch 2 Batch 950 Loss 0.6811\n",
            "Epoch 2 Batch 1000 Loss 0.6608\n",
            "Saving checkpoint for epoch 2 at /content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/ckpt-8\n",
            "Epoch 2 Batch 1050 Loss 0.6433\n",
            "Epoch 2 Batch 1100 Loss 0.6330\n",
            "Epoch 2 Batch 1150 Loss 0.6185\n",
            "Epoch 2 Batch 1200 Loss 0.6060\n",
            "Epoch 2 Batch 1250 Loss 0.6054\n",
            "Epoch 2 Batch 1300 Loss 0.6103\n",
            "Epoch 2 Batch 1350 Loss 0.6206\n",
            "Epoch 2 Batch 1400 Loss 0.6190\n",
            "Epoch 2 Batch 1450 Loss 0.6142\n",
            "Epoch 2 Batch 1500 Loss 0.6103\n",
            "Saving checkpoint for epoch 2 at /content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/ckpt-9\n",
            "Epoch 2 Batch 1550 Loss 0.6085\n",
            "Epoch 2 Batch 1600 Loss 0.6107\n",
            "Epoch 2 Batch 1650 Loss 0.6145\n",
            "Epoch 2 Batch 1700 Loss 0.6196\n",
            "Epoch 2 Batch 1750 Loss 0.6227\n",
            "Epoch 2 Batch 1800 Loss 0.6281\n",
            "Epoch 2 Batch 1850 Loss 0.6244\n",
            "Epoch 2 Batch 1900 Loss 0.6205\n",
            "Epoch 2 Batch 1950 Loss 0.6219\n",
            "Time taken for 1 epoch: 890.9959127902985 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 0.5132\n",
            "Saving checkpoint for epoch 3 at /content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/ckpt-10\n",
            "Epoch 3 Batch 50 Loss 0.9929\n",
            "Epoch 3 Batch 100 Loss 1.0604\n",
            "Epoch 3 Batch 150 Loss 1.0698\n",
            "Epoch 3 Batch 200 Loss 1.0946\n",
            "Epoch 3 Batch 250 Loss 0.9966\n",
            "Epoch 3 Batch 300 Loss 0.9494\n",
            "Epoch 3 Batch 350 Loss 0.9410\n",
            "Epoch 3 Batch 400 Loss 0.8994\n",
            "Epoch 3 Batch 450 Loss 0.8523\n",
            "Epoch 3 Batch 500 Loss 0.8340\n",
            "Saving checkpoint for epoch 3 at /content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/ckpt-11\n",
            "Epoch 3 Batch 550 Loss 0.8047\n",
            "Epoch 3 Batch 600 Loss 0.7930\n",
            "Epoch 3 Batch 650 Loss 0.7857\n",
            "Epoch 3 Batch 700 Loss 0.7647\n",
            "Epoch 3 Batch 750 Loss 0.7353\n",
            "Epoch 3 Batch 800 Loss 0.7166\n",
            "Epoch 3 Batch 850 Loss 0.7079\n",
            "Epoch 3 Batch 900 Loss 0.6994\n",
            "Epoch 3 Batch 950 Loss 0.6853\n",
            "Epoch 3 Batch 1000 Loss 0.6619\n",
            "Saving checkpoint for epoch 3 at /content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/ckpt-12\n",
            "Epoch 3 Batch 1050 Loss 0.6496\n",
            "Epoch 3 Batch 1100 Loss 0.6335\n",
            "Epoch 3 Batch 1150 Loss 0.6204\n",
            "Epoch 3 Batch 1200 Loss 0.6076\n",
            "Epoch 3 Batch 1250 Loss 0.6032\n",
            "Epoch 3 Batch 1300 Loss 0.6077\n",
            "Epoch 3 Batch 1350 Loss 0.6211\n",
            "Epoch 3 Batch 1400 Loss 0.6170\n",
            "Epoch 3 Batch 1450 Loss 0.6130\n",
            "Epoch 3 Batch 1500 Loss 0.6094\n",
            "Saving checkpoint for epoch 3 at /content/drive/MyDrive/BERT_UDEMY/models/ckpt_bert_squad/ckpt-13\n",
            "Epoch 3 Batch 1550 Loss 0.6106\n",
            "Epoch 3 Batch 1600 Loss 0.6102\n",
            "Epoch 3 Batch 1650 Loss 0.6172\n",
            "Epoch 3 Batch 1700 Loss 0.6226\n",
            "Epoch 3 Batch 1750 Loss 0.6259\n",
            "Epoch 3 Batch 1800 Loss 0.6281\n",
            "Epoch 3 Batch 1850 Loss 0.6246\n",
            "Epoch 3 Batch 1900 Loss 0.6190\n",
            "Epoch 3 Batch 1950 Loss 0.6273\n",
            "Time taken for 1 epoch: 893.3164660930634 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6WquTCiIR7t"
      },
      "source": [
        "# Stage 5: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDIlHd5Tos6C"
      },
      "source": [
        "## Prepare evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU7_AyTIpTTJ"
      },
      "source": [
        "Get the dev set in the session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGKl84s5WrhD"
      },
      "source": [
        "eval_examples = read_squad_examples(\n",
        "    \"/content/drive/MyDrive/Datasets/Squad_1.0/dev-v1.1.json\",\n",
        "    is_training=False,\n",
        "    version_2_with_negative=False)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAEUcZDSpYLD"
      },
      "source": [
        "Define the function that will write the tf_record file for the dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCVmIgnEo83o"
      },
      "source": [
        "eval_writer = FeatureWriter(\n",
        "    filename=os.path.join(\"/content/drive/MyDrive/Datasets/Squad_1.0/\",\n",
        "                          \"eval.tf_record\"),\n",
        "    is_training=False)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8aSLFdmp71I"
      },
      "source": [
        "Create a tokenizer for future information needs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH5exQ7KwnuH"
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdHudjJ_qAzo"
      },
      "source": [
        "Define the function that add the features (feature is a protocol in tensorflow) to our eval_features list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmQ5GtoTxRjU"
      },
      "source": [
        "def _append_feature(feature, is_padding):\n",
        "    if not is_padding:\n",
        "        eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLAcwCiaqHi_"
      },
      "source": [
        "Create the eval features and the writes the tf.record file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz7kGYmUwGQb"
      },
      "source": [
        "eval_features = []\n",
        "dataset_size = convert_examples_to_features(\n",
        "    examples=eval_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=384,\n",
        "    doc_stride=128,\n",
        "    max_query_length=64,\n",
        "    is_training=False,\n",
        "    output_fn=_append_feature,\n",
        "    batch_size=4)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpZfwPEwMabx"
      },
      "source": [
        "eval_writer.close()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbKhx3zuq844"
      },
      "source": [
        "Load the ready-to-be-used dataset to our session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUqYvG5TxctF"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "eval_dataset = create_squad_dataset(\n",
        "    \"/content/drive/MyDrive/Datasets/Squad_1.0/eval.tf_record\",\n",
        "    384,#input_meta_data['max_seq_length'],\n",
        "    BATCH_SIZE,\n",
        "    is_training=False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRbKrFYoo8e8"
      },
      "source": [
        "## Making the predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyckEWDbrLEX"
      },
      "source": [
        "Defines a certain type of collection (like a dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyWOUKaDP0H0"
      },
      "source": [
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28abKVvqrRa4"
      },
      "source": [
        "Returns each element of batched output at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BScaA0SZQgQW"
      },
      "source": [
        "def get_raw_results(predictions):\n",
        "    for unique_ids, start_logits, end_logits in zip(predictions['unique_ids'],\n",
        "                                                    predictions['start_logits'],\n",
        "                                                    predictions['end_logits']):\n",
        "        yield RawResult(\n",
        "            unique_id=unique_ids.numpy(),\n",
        "            start_logits=start_logits.numpy().tolist(),\n",
        "            end_logits=end_logits.numpy().tolist())"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiLOOmnLre5C"
      },
      "source": [
        "Let's make our predictions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqD78Xdjrvpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af9ae08-a5b4-4b6f-cbd3-4eb128942bc8"
      },
      "source": [
        "all_results = []\n",
        "for count, inputs in enumerate(eval_dataset):\n",
        "    x, _ = inputs\n",
        "    unique_ids = x.pop(\"unique_ids\")\n",
        "    start_logits, end_logits = bert_squad(x, training=False)\n",
        "    output_dict = dict(\n",
        "        unique_ids=unique_ids,\n",
        "        start_logits=start_logits,\n",
        "        end_logits=end_logits)\n",
        "    for result in get_raw_results(output_dict):\n",
        "        all_results.append(result)\n",
        "    if count % 100 == 0:\n",
        "        print(\"{}/{}\".format(count, 2709))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/2709\n",
            "100/2709\n",
            "200/2709\n",
            "300/2709\n",
            "400/2709\n",
            "500/2709\n",
            "600/2709\n",
            "700/2709\n",
            "800/2709\n",
            "900/2709\n",
            "1000/2709\n",
            "1100/2709\n",
            "1200/2709\n",
            "1300/2709\n",
            "1400/2709\n",
            "1500/2709\n",
            "1600/2709\n",
            "1700/2709\n",
            "1800/2709\n",
            "1900/2709\n",
            "2000/2709\n",
            "2100/2709\n",
            "2200/2709\n",
            "2300/2709\n",
            "2400/2709\n",
            "2500/2709\n",
            "2600/2709\n",
            "2700/2709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjQ6kIqGriHr"
      },
      "source": [
        "Write the predictions in a json file that will work with the evaluation script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esLdRf7uM3Lz"
      },
      "source": [
        "output_prediction_file = \"/content/drive/MyDrive/Datasets/Squad_1.0/predictions.json\"\n",
        "output_nbest_file = \"/content/drive/MyDrive/Datasets/Squad_1.0/nbest_predictions.json\"\n",
        "output_null_log_odds_file = \"/content/drive/MyDrive/Datasets/Squad_1.0/null_odds.json\"\n",
        "\n",
        "write_predictions(\n",
        "    eval_examples,\n",
        "    eval_features,\n",
        "    all_results,\n",
        "    20,\n",
        "    30,\n",
        "    True,\n",
        "    output_prediction_file,\n",
        "    output_nbest_file,\n",
        "    output_null_log_odds_file,\n",
        "    verbose=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eaIHyDIYHHx"
      },
      "source": [
        "## Home-made prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0F4l5h8Zdha"
      },
      "source": [
        "### Input dict creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrhHzx7ycXXo"
      },
      "source": [
        "We will concatenate the question and the context, separated by a `[\"SEP\"]`, after tokenization, as it has been made for the training set.\n",
        "\n",
        "The important thing is that we want our answer to start with a real word and to end with a real word. The word \"ecologically\" being tokenized as `[\"ecological\", \"##ly\"]`, if the ending token is `[\"ecological\"]` we want to use \"ecologically\" as the ending word (same thing if the ending token is `[\"##ly\"]`). That is why we first split our context into words, and then into tokens, remembering to which word each token belongs to (see the `tokenize_context()` function)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tuNXt98Zm4u"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBjGoQ_wfmml"
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f_fCe_hLC12"
      },
      "source": [
        "def is_whitespace(c):\n",
        "    '''\n",
        "    Tell if a chain of characters corresponds to a whitespace or not.\n",
        "    '''\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZA4TYnxLGVT"
      },
      "source": [
        "def whitespace_split(text):\n",
        "    '''\n",
        "    Take a text and return a list of \"words\" by splitting it according to\n",
        "    whitespaces.\n",
        "    '''\n",
        "    doc_tokens = []\n",
        "    prev_is_whitespace = True\n",
        "    for c in text:\n",
        "        if is_whitespace(c):\n",
        "            prev_is_whitespace = True\n",
        "        else:\n",
        "            if prev_is_whitespace:\n",
        "                doc_tokens.append(c)\n",
        "            else:\n",
        "                doc_tokens[-1] += c\n",
        "            prev_is_whitespace = False\n",
        "    return doc_tokens"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsfzt3GUNWQK"
      },
      "source": [
        "def tokenize_context(text_words):\n",
        "    '''\n",
        "    Take a list of words (returned by whitespace_split()) and tokenize each word\n",
        "    one by one. Also keep track, for each new token, of its original word in the\n",
        "    text_words parameter.\n",
        "    '''\n",
        "    text_tok = []\n",
        "    tok_to_word_id = []\n",
        "    for word_id, word in enumerate(text_words):\n",
        "        word_tok = tokenizer.tokenize(word)\n",
        "        text_tok += word_tok\n",
        "        tok_to_word_id += [word_id]*len(word_tok)\n",
        "    return text_tok, tok_to_word_id"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8qreqEURUOP"
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n",
        "    return seg_ids"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2sPGXxsYUsY"
      },
      "source": [
        "def create_input_dict(question, context):\n",
        "    '''\n",
        "    Take a question and a context as strings and return a dictionary with the 3\n",
        "    elements needed for the model. Also return the context_words, the\n",
        "    context_tok to context_word ids correspondance and the length of\n",
        "    question_tok that we will need later.\n",
        "    '''\n",
        "    question_tok = tokenizer.tokenize(my_question)\n",
        "\n",
        "    context_words = whitespace_split(context)\n",
        "    context_tok, context_tok_to_word_id = tokenize_context(context_words)\n",
        "\n",
        "    input_tok = question_tok + [\"[SEP]\"] + context_tok + [\"[SEP]\"]\n",
        "    input_tok += [\"[PAD]\"]*(384-len(input_tok)) # in our case the model has been\n",
        "                                                # trained to have inputs of length max 384\n",
        "    input_dict = {}\n",
        "    input_dict[\"input_word_ids\"] = tf.expand_dims(tf.cast(get_ids(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_mask\"] = tf.expand_dims(tf.cast(get_mask(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_type_ids\"] = tf.expand_dims(tf.cast(get_segments(input_tok), tf.int32), 0)\n",
        "\n",
        "    return input_dict, context_words, context_tok_to_word_id, len(question_tok)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAnaCZWTZpWT"
      },
      "source": [
        "#### Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQlM-M8rMklA"
      },
      "source": [
        "my_context = '''Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions.'''"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL_cE-o0U8mx"
      },
      "source": [
        "Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeB29SCNNQ1M"
      },
      "source": [
        "#my_question = '''What philosophy of thought addresses wealth inequality?'''\n",
        "my_question = '''What are examples of economic actors?'''\n",
        "#my_question = '''In a market economy, what is inequality a reflection of?'''"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v5nLMNjZufe"
      },
      "source": [
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT066rMtZ65X"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcJgDa9gVShl"
      },
      "source": [
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhdGlIo5Z9IZ"
      },
      "source": [
        "### Interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQJMBkVLd9wp"
      },
      "source": [
        "We remove the ids corresponding to the question and the `[\"SEP\"]` token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfwBJfsSTwRn"
      },
      "source": [
        "start_logits_context = start_logits.numpy()[0, question_tok_len+1:]\n",
        "end_logits_context = end_logits.numpy()[0, question_tok_len+1:]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te1u6iZAawYf"
      },
      "source": [
        "First easy interpretation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQJ8tp-1WvI4"
      },
      "source": [
        "start_word_id = context_tok_to_word_id[np.argmax(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[np.argmax(end_logits_context)]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3PHA84rarse"
      },
      "source": [
        "\"Advanced\" - making sure that the start of the answer is before the end:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFZ2fUiRU_M"
      },
      "source": [
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9KEiFHPXXeM"
      },
      "source": [
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJDBL8KKa6NP"
      },
      "source": [
        "Final answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Y3WDz0XwAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d570d2-b880-48a3-e021-dd2bf11b7255"
      },
      "source": [
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "print(\"The answer to:\\n\" + my_question + \"\\nis:\\n\" + predicted_answer)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer to:\n",
            "What are examples of economic actors?\n",
            "is:\n",
            "(worker, capitalist/business owner, landlord).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYGSk_5OSYUk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "30e98e58-eb30-4a0e-b7a0-13a20acabe8a"
      },
      "source": [
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>WHAT ARE EXAMPLES OF ECONOMIC ACTORS?</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<blockquote> Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor <mark>(worker, capitalist/business owner, landlord).</mark> Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions. </blockquote>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brg0AXkA0r74"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}